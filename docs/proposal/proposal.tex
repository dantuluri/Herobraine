\documentclass[11pt]{article}
%
\usepackage{abstract,amsmath,amssymb,latexsym}
\usepackage{enumitem,epsf}
\usepackage[stable]{footmisc}
\usepackage{fullpage,tikz,float}
\usepackage{caption}
\usepackage[numbers]{natbib}
\usepackage[pdftex,colorlinks]{hyperref}
\usepackage{array, booktabs}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage[TS1,T1]{fontenc}


\newcommand{\foo}{\makebox[0pt]{\textbullet}\hskip-0.5pt\vrule width 1pt\hspace{\labelsep}}


% locally defined macros
\usepackage{macros}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following for revealing TODOs and appendices
% Options are: \draftfalse or \drafttrue
\newif\ifdraft
\draftfalse

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{
 \begin{minipage}[c]{1.05\textwidth}
 	\centerline{Massively Multiagent Hierarchical Inverse}
 	\centerline{Reinforcement Learning in Open-world Environments}
 \end{minipage}
}

\author{
	\vspace{1cm}
	William H. Guss\thanks{Carnegie Mellon University, Machine Learning Department.}\\[-1cm]
	wguss@cs.cmu.edu \and
	Ruslan Salakhutdinov\footnotemark[1] \\
	rsalakhu@cs.cmu.edu
}

\begin{document}

\maketitle
\thispagestyle{empty}

\begin{abstract}
	\todo{
Will, can you put together a quick 1.5-2 page proposal on inverse RL, Frank from US Army is saying that there is a potential to fund this kind of research: working with simulation environments, working on agents that can plan, communicate, solve slam, build strategies, do reward shaping via inverse RL, etc. They basically want to emphasize: 1, Metrics and Experiments. 2. Describing state-of-the-art in this area, and why what we are proposing is elite and new. Looks like US Army likes very ambitious projects. I will this together into one coherent white paper and we can see whether they would be willing to fund us. If not, we can take the same proposal to someone else as well.
}
\end{abstract}

\setcounter{page}{1}


\section{Introduction}

\todo{Introduction to inverse reinforcement learning; what has it done?}

Over recent years inverse reinforcement learning (IRL) has revolutionized the state of the art in apprenticeship learning, cooperative and adversarial modeling, and the modeling of intent in human and animal behaviour\todo{cite}. At its core, IRL solves the problem of learning expert policies indirectly: in direct juxtaposition to behavioral cloning IRL learns the reward function of an expert agent and then produces a policy which maximizes that reward. In this regime, the resultant policy is often far more interpratable, robust, and sample efficient than that of behavioural cloning. 

Mechanically, inverse reinforcement learning is optimal not only for cloning expert policies in applications where thousands if not millions of demonstrations are not possible, but also as a substitute for traditional reinforcement learning when exploration is extremely expensive. For example, in robotic manipulation tasks, where typical $\epsilon$-greedy exploration polices would result in potential damage to the robot, apprenticeship learning via IRL is a powerful alternative. Furthermore, by learning the reward function directly, IRL is an effective, interpritable forecasting mechanism in tasks such as epidimiological modeling, traffic prediciton, and first person activity forecasting.\todo{cite}


Despite its numerous applications and avantages, IRL is an underconstrained in the sense  

Formally, let $R$

Due to the power of IRL, practitioners have been able to predict traffic patterns, I don't really know what I'm saying with this. So I think the best strategy is to write as much as possible and then cut. How about that. Bullshit whatever but spin this into a narative that you'd tell
Core to the success of IRL is not only its ability to recapitulate expert policies from a relatively low number of samples but also 

\todo{Provide a slightly more formal perspectrive to motivate why these methods could be interesting?}


\todo{Introduce the problem statement, what is the problem of massively multiagent hierarchical reinforcement learning in open-world environments and why would this be pertinent.}

\todo{Introduce the implications of solving the MMHIRL problem in open-world environments, potentially reference the taxi problem.}


\section{Our Approach}

\todo{To solve the problem of MMHIRL we will introduce methods which take advantage of the local connectedness of parameterized policy and reward space.}

\todo{}


\subsection{Experiments}

\subsection{Metrics}





\bibliographystyle{plainnat}
\bibliography{bib}

\end{document}
