\documentclass[11pt]{article}
%
\usepackage{abstract,amsmath,amssymb,latexsym}
\usepackage{enumitem,epsf}
\usepackage[stable]{footmisc}
\usepackage{fullpage,tikz,float}
\usepackage{caption}
\usepackage[numbers]{natbib}
\usepackage[pdftex,colorlinks]{hyperref}
\usepackage{array, booktabs}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage[TS1,T1]{fontenc}


\newcommand{\foo}{\makebox[0pt]{\textbullet}\hskip-0.5pt\vrule width 1pt\hspace{\labelsep}}


% locally defined macros
\usepackage{macros}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following for revealing TODOs and appendices
% Options are: \draftfalse or \drafttrue
\newif\ifdraft
\draftfalse

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{
 \begin{minipage}[c]{1.05\textwidth}
 	\centerline{Massively Multiagent Hierarchical Inverse}
 	\centerline{Reinforcement Learning in Open-world Environments}
 \end{minipage}
}

\author{
	\vspace{1cm}
	William H. Guss\thanks{Carnegie Mellon University, Machine Learning Department.}\\[-1cm]
	wguss@cs.cmu.edu \and
	Ruslan Salakhutdinov\footnotemark[1] \\
	rsalakhu@cs.cmu.edu
}

\begin{document}

\maketitle
\thispagestyle{empty}


\setcounter{page}{1}


\section{Introduction}

Over recent years inverse reinforcement learning (IRL) has revolutionized the state of the art in apprenticeship learning, cooperative and adversarial modeling, and the modeling of intent in human and animal behaviour\todo{cite}. At its core, IRL solves the problem of learning expert policies indirectly: in direct juxtaposition to behavioral cloning IRL learns the reward function of an expert agent and then produces a policy which maximizes that reward. In this regime, the resultant policy is often far more interpratable, robust, and sample efficient than that of behavioural cloning. 

Mechanically, inverse reinforcement learning is optimal not only for cloning expert policies in applications where thousands if not millions of demonstrations are not possible, but also as a substitute for traditional reinforcement learning when exploration is extremely expensive. For example, in robotic manipulation tasks, where typical $\epsilon$-greedy exploration polices would result in potential damage to the robot, apprenticeship learning via IRL is a powerful alternative. Furthermore, by learning the reward function directly, IRL is an effective, interpritable forecasting mechanism in tasks such as epidimiological modeling, traffic prediciton, and first person activity forecasting.\todo{cite}


Despite its numerous applications and avantages, IRL is an underconstrained optimzation problem; in particular, there are potentially inifinitely many reward functions which explain an expert policies behaviour. To see this formally, let $(S,A,T,\gamma,D)$ be a rewardless Markov deciion process (MDP) with state space $S$, action space $A$, state-to-state transition function $T$, $\gamma$ some marginal utility discount, and $D$ an initial state distribution. Given some expert policy $\pi^*(a | s )$, where $a \in A, s \in S$, inverse reinforcement learning aims to find a reward function $R: S \times A \to \mathbb{R}$ such that 
\begin{equation}
	\pi^* = \arg \max_{\pi} \mathbb{E}\left[\sum_{t=1}^\infty \gamma^t  R(s_t, a_t)\right]
\end{equation}
where $s_t \sim T(s_{t-1}, a_{t-1}), a_t \sim \pi(\cdot | s_t),$ and $s_0 \sim D.$ In this setup, it is clear that degenerate reward functions such as $R = 0$ suffice in explainig $\pi^*.$ Constraining the space of candidate reward functions has therefore become central to the application of inverse reinforcement learning. 


Formally, let $R$

Due to the power of IRL, practitioners have been able to predict traffic patterns, I don't really know what I'm saying with this. So I think the best strategy is to write as much as possible and then cut. How about that. Bullshit whatever but spin this into a narative that you'd tell
Core to the success of IRL is not only its ability to recapitulate expert policies from a relatively low number of samples but also 

\todo{Provide a slightly more formal perspectrive to motivate why these methods could be interesting?}


\todo{Introduce the problem statement, what is the problem of massively multiagent hierarchical reinforcement learning in open-world environments and why would this be pertinent.}

\todo{Introduce the implications of solving the MMHIRL problem in open-world environments, potentially reference the taxi problem.}


\section{Our Approach}

\todo{To solve the problem of MMHIRL we will introduce methods which take advantage of the local connectedness of parameterized policy and reward space.}

\todo{}


\subsection{Experiments}

\subsection{Metrics}





\bibliographystyle{plainnat}
\bibliography{bib}

\end{document}
